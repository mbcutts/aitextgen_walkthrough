{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Processing Walkthrough\n",
        "Please make a copy of this notebook to work on in colab or your favorite jupyter environment. \n",
        "\n",
        "Note that this is an advanced tutorial walking through a specific use case of aitextgen. It is highly recommended that you visit Max Woolf's [tutorial](https://colab.research.google.com/drive/144MdX5aLqrQ3-YW-po81CQMrD6kpgpYh?usp=sharing#scrollTo=LdpZQXknFNY3) on aitextgen custom model training before continuing forward with this tutorial.\n",
        "\n",
        "If you find any typos or have questions, enhancement requests, or suggestions, please reach out to [mitchellbcutts@gmail.com](mailto:mitchellbcutts@gmail.com).\n"
      ],
      "metadata": {
        "id": "c0o8Bl6QdV5p"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "BefBctNz9UJX"
      },
      "source": [
        "# Setup\n",
        "This tutorial will walk you through the python and machine learning side of a GPT text generation project. The dataset for this project is taken from [kaggle, you can click here to check it out](https://www.kaggle.com/neisse/scrapped-lyrics-from-6-genres?select=artists-data.csv). The overall project this tutorial will build is a *Rock Lyric Generator*!!!\n",
        "\n",
        "Disclaimer: This tutorial was built using three sources that are worth checking out:\n",
        "  - Max Woolf's [tutorial](https://colab.research.google.com/drive/144MdX5aLqrQ3-YW-po81CQMrD6kpgpYh?usp=sharing#scrollTo=LdpZQXknFNY3) on aitextgen custom model training\n",
        "  - Max Woolf's [tutorial](https://colab.research.google.com/drive/15qBZx5y9rdaQSyWpsreMDnTiZ5IlN0zD?usp=sharing) on aitextgen GPT-2 fine tuning\n",
        "  - Francois St-Amant's [tutorial](https://towardsdatascience.com/how-to-fine-tune-gpt-2-for-text-generation-ae2ea53bc272) on Fine Tuning GPT-2 and evaluation\n",
        "\n",
        "For CPU training see: https://github.com/minimaxir/aitextgen/blob/master/notebooks/training_hello_world.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "5Zd1lD3rRQ-m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0e0048f-8cc7-4502-da29-836580e3c3e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9Fx9Rdq93mM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4351046-1cd1-43f2-c6e5-69ff67e77efb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/drive/MyDrive/LYRICBOT'\n",
            "/content\n",
            "\u001b[K     |████████████████████████████████| 572 kB 5.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 54.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 87 kB 6.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 527 kB 52.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 134 kB 57.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 952 kB 55.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 397 kB 58.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 829 kB 56.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 53.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 53.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 67 kB 6.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 46.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 39.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 59.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 61.1 MB/s \n",
            "\u001b[?25h  Building wheel for aitextgen (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/LYRICBOT\n",
        "!pip3 install -q aitextgen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whTZ3ixr9JjF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b57ed623-8811-4bc1-d7d5-f40e20560a20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "03/18/2022 19:52:01 — INFO — numexpr.utils — NumExpr defaulting to 2 threads.\n"
          ]
        }
      ],
      "source": [
        "#imports\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "import logging\n",
        "logging.basicConfig(\n",
        "        format=\"%(asctime)s — %(levelname)s — %(name)s — %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO\n",
        "    )\n",
        "from sklearn.model_selection import train_test_split\n",
        "from aitextgen import aitextgen\n",
        "from aitextgen.TokenDataset import TokenDataset\n",
        "from aitextgen.utils import build_gpt2_config\n",
        "from aitextgen.tokenizers import train_tokenizer\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "6vfXI7ps-U8p"
      },
      "source": [
        "# Linux, Command Line and Pandas Dataset Trimming\n",
        "Since Colab and Cocalc run on Linux, I'm going to take advantage of some Linux commands to download my dataset.\n",
        "\n",
        "I then will use pandas to subset my dataset as shown below:\n",
        "\n",
        "- _original dataset:_ over 190000 song lyrics that are in multiple languages and genres \n",
        "\n",
        "- _finalized dataset:_ about 94000 song lyrics that are in english and are of the \"rock\" genre. \n",
        "\n",
        "Now our model will have much more specific data to train upon and we can expect it to output english language based on rock songs.\n",
        "\n",
        "**Note: because this is a comprehensive tutorial, I wanted to include some preprocessing steps and options. If your data is simply a text file, you will not need these steps.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pvNRGVQ-ODB"
      },
      "outputs": [],
      "source": [
        "# will download the dataset files to your local repository! uncomment this the first time you are using this notebook.\n",
        "!gdown 13WCZxed3GcGMvcGX69y5D2rfhFtA8w0a\n",
        "# will unzip the zip file of the dataset. uncomment this the first time you are using this notebook.\n",
        "!unzip lyrics_dataset.zip\n",
        "%ls #use this to check if they are there"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nA5MrjoZ-lNT"
      },
      "outputs": [],
      "source": [
        "artists = pd.read_csv(\"artists-data.csv\")\n",
        "lyrics = pd.read_csv(\"lyrics-data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7LsapUyMpxR"
      },
      "outputs": [],
      "source": [
        "artists.dropna(axis=0)\n",
        "artists = artists[artists[\"Genres\"].str.contains(\"Rock\", na=False)]\n",
        "artists.head() #now we have all the artists that make rock music"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wX376HQmMpxS"
      },
      "outputs": [],
      "source": [
        "artists[\"Link\"].astype('string')\n",
        "artist_links = artists.Link.values.tolist() #creating a list of artist names that make rock music so we can cross reference the lyrics data to see if they belong to rock or an adjacent genre. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qyq9eQOH-tL9"
      },
      "outputs": [],
      "source": [
        "lyrics.dropna(axis=0)\n",
        "lyrics = lyrics[lyrics['language'] == 'en']\n",
        "lyrics.ALink.astype(\"string\")\n",
        "lyrics.head() #now we have all english lyrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mktvObBfMpxV"
      },
      "outputs": [],
      "source": [
        "master_lyrics = lyrics[lyrics[\"ALink\"].isin(artist_links)]\n",
        "master_lyrics.index #our improved dataset is 94992 rock or rock adjacent songs."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "master_lyrics.head()"
      ],
      "metadata": {
        "id": "WbFhyd8vR4uI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "BOrpbWo-MpxW"
      },
      "source": [
        "# Train/Test Split and Preprocessing for Training\n",
        "Now that we have our data frame subsetted into 94992 samples of rock lyrics, we will perform a train/test split on our data so we can perform BLEU metric evaluation later in the project.\n",
        "\n",
        "Note that performing a train/test split is not something you will always have to do, and you should avoid this if your dataset is simply a text file and can't be broken up into several observations (such as the classic example of training on [the complete shakespeare works](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt)). If you don't perform a train/test split, note that you will be evaluating your model qualitatively by human inspection. \n",
        "\n",
        "This dataset is a good candidate to train/test split because it has several hundred different instances and is not just a single text file. To perform the split effectively to where we will be able to train, we will also save the  training data to a `TokenDataset` type from aitextgen which will encode the data for training!\n",
        "\n",
        "**Notes:** we will be manipulating the test dataframe later when we get to evaluation. For now, we will leave it as is. Also, if your dataset is a text file, you will not need to perform these steps and will perform manual evaluation. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "id": "loLBHzlRXyix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "master_lyrics.Lyric.astype(\"string\")\n",
        "train, test = train_test_split(master_lyrics, test_size=0.002) #we will use sklearn's train/test split function to split our data 75/25\n",
        "print(\"training shape: \", train.shape, \" testing shape: \", test.shape)\n",
        "\n",
        "training_samples = train.Lyric.values.tolist()\n",
        "# print(\"\\n\\nprinting part of a training sample: \\n\\n\" + training_samples[0][15:240]) # if you want to inspect output"
      ],
      "metadata": {
        "id": "x0D8ETBeYbux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "B9iXhCpEABGx"
      },
      "source": [
        "# Training\n",
        "\n",
        "Now here is where the magic happens! This next section will guide you through training your model. \n",
        "\n",
        "The main decision you will have to make is whether you will train your own custom tokenizer + model from scratch or simply tune an existing model to fit your needs. Here is a snippet from `aitextgen`'s documentation that should help you decide.\n",
        "    \n",
        "    The original GPT-2 model released by OpenAI was trained on English webpages linked to from Reddit. \n",
        "    It has a strong bias toward longform content (multiple paragraphs). \n",
        "    \n",
        "    If that is *not* your use case, you may get a better generation quality *and* speed by training your own model and Tokenizer.\n",
        "    Examples of good use cases for training your own tokenizer:\n",
        "    \n",
        "    - Short-form content (e.g. Tweets, Reddit post titles)\n",
        "    - Non-English Text\n",
        "    - Heavily Encoded Text\n",
        "    \n",
        "    It still will require a *massive* amount of training time (several hours) but will be more flexible.\n",
        "\n",
        "In this tutorial, we will cover both [training our own custom model and tokenizer](https://docs.aitextgen.io/tutorials/model-from-scratch/) as well as [fine tuning a GPT-2 model](https://colab.research.google.com/drive/15qBZx5y9rdaQSyWpsreMDnTiZ5IlN0zD?usp=sharing). Note that you only need to do one in your project. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIvgtsyumKcA"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi -L #figure out what type of GPU we are using"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training your own custom model and tokenizer. \n",
        "Follow these steps to train your own custom model and tokenizer. Remember, here are the use cases for training your own tokenizer.\n",
        "\n",
        "- Short-form content (e.g. Tweets, Reddit post titles)\n",
        "- Non-English Text\n",
        "- Heavily Encoded Text\n",
        "\n",
        "**IMPORTANT NOTE FOR TIME MANAGEMENT:** Note that this will require a *massive* amount of training time (several hours) but will be more flexible so **if you are working on short-form text generation, use the fine-tuning method first** (next section) so you at very least have a working model before taking all the time needed to train your own tokenizer and custom model.\n",
        "\n"
      ],
      "metadata": {
        "id": "CUoLWvKPb7kF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the Tokenizer\n",
        "The `train_tokenizer()` function wraps the training method for the tokenizer package from Huggingface. We are going to need to give it a file to train on, so we will simply export our training lyrics to `.txt` format in order to satisfy the `train_tokenizer()` argument requirements.\n",
        "\n",
        "After the training is completed, this will save one file: aitextgen.tokenizer.json, which is needed to rebuild the tokenizer (in other words save it for later)."
      ],
      "metadata": {
        "id": "bmYXfFRqdi8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = \"/content/drive/MyDrive/LYRICBOT/tokenizer_input.txt\"\n",
        "train.Lyric.astype(\"string\")\n",
        "with open(file_name, 'a') as f:\n",
        "    dfAsString = train[\"Lyric\"].to_string(header=False, index=False)\n",
        "    f.write(dfAsString)"
      ],
      "metadata": {
        "id": "IKdzDagPbv4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tokenizer(file_name)\n",
        "tokenizer_file = \"aitextgen.tokenizer.json\""
      ],
      "metadata": {
        "id": "kDiQQ2yBvhQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset = TokenDataset(texts=training_samples, tokenizer_file=tokenizer_file, save_cache=True)\n",
        "training_file = \"/content/drive/MyDrive/LYRICBOT/dataset_cache.tar.gz\"\n",
        "# training_dataset = TokenDataset(\"dataset_cache.tar.gz\", from_cache=True) #if you want to load in for later!"
      ],
      "metadata": {
        "id": "W15rodQu2q2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Specify a model configuration\n",
        "You can use build_gpt2_config to specify a model configuration. You most likely will want to adjust max_length (context window size) and n_embd (embedding size). The config used here is the one used to build a demo Reddit model; I recommend you experiment with these parameters and read up about context windows and embedding size before camp!"
      ],
      "metadata": {
        "id": "h2Py_KCjdSwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = build_gpt2_config(vocab_size=5000, max_length=32, dropout=0.0, n_embd=256, n_layer=8, n_head=8)\n",
        "config"
      ],
      "metadata": {
        "id": "-y_iwm2db45p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instantiating Your Custom GPT-2 Model"
      ],
      "metadata": {
        "id": "_OcGt66pgsis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ai_custom = aitextgen(config=config, tokenizer_file=tokenizer_file)"
      ],
      "metadata": {
        "id": "zFS1W35ggrPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Your Custom GPT-2 Model\n",
        "\n",
        "The next cell will start the actual training of GPT-2 in aitextgen. It runs for `num_steps`, and a progress bar will appear to show training progress, current loss (the lower the better the model), and average loss (to give a sense on loss trajectory).\n",
        "\n",
        "The model will be saved every `save_every` steps in `trained_model` by default, and when training completes. If you mounted your Google Drive, the model will _also_ be saved there in a unique folder.\n",
        "\n",
        "The training might time out after 4ish hours; if you did not mount to Google Drive, make sure you end training and save the results so you don't lose them! (if this happens frequently, you may want to consider using [Colab Pro](https://colab.research.google.com/signup))\n",
        "\n",
        "Important parameters for `train()`:\n",
        "\n",
        "- **`line_by_line`**: Set this to `True` if the input text file is a single-column CSV, with one record per row. aitextgen will automatically process it optimally.\n",
        "- **`from_cache`**: If you compressed your dataset locally (as noted in the previous section) and are using that cache file, set this to `True`.\n",
        "- **`num_steps`**: Number of steps to train the model for.\n",
        "- **`generate_every`**: Interval of steps to generate example text from the model; good for qualitatively validating training.\n",
        "- **`save_every`**: Interval of steps to save the model: the model will be saved in the VM to `/trained_model`.\n",
        "- **`save_gdrive`**: Set this to `True` to copy the model to a unique folder in your Google Drive, if you have mounted it in the earlier cells\n",
        "- **`batch_size`**: Batch size of the model training; setting it too high will cause the GPU to go OOM. _Unlike finetuning, since you are using a small model, you can massively increase the batch size to normalize the training_.\n",
        "- **`fp16`**: Enables half-precision training for faster/more memory-efficient training. Only works on a T4 or V100 GPU.\n",
        "\n",
        "Here are other important parameters for `train()` that are useful but you likely do not need to change.\n",
        "\n",
        "- **`learning_rate`**: Learning rate of the model training.\n"
      ],
      "metadata": {
        "id": "tgABTgs5gx48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ai_custom.train(training_file,\n",
        "                line_by_line=False,\n",
        "                from_cache=True,\n",
        "                num_steps=10000,\n",
        "                generate_every=500,\n",
        "                save_every=500,\n",
        "                save_gdrive=True,\n",
        "                learning_rate=1e-3,\n",
        "                output_dir = \"/content/drive/MyDrive/LYRICBOT/custom_model\",\n",
        "                batch_size=256)"
      ],
      "metadata": {
        "id": "G7p-YF30iDFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "9rLbA7IflDxg"
      },
      "source": [
        "## Fine-Tuning GPT-2  \n",
        "Follow these steps to fine tune an existing model. This will perform best on longer texts, but it can be used as a baseline model for shorter texts as well. \n",
        "\n",
        "For this project, training a custom model is probably a better idea given that this is short instances of text. Think about what would be best for your project!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yum4XcxWAEzG"
      },
      "outputs": [],
      "source": [
        "training_dataset = TokenDataset(texts=training_samples, save_cache=True) #not using a custom tokenizer so redefining this variable to match that.\n",
        "training_file = \"/content/drive/MyDrive/LYRICBOT/dataset_cache.tar.gz\"\n",
        "'''\n",
        "Commented Below are some options for models you can use on this project. We have found distilgpt2 to be faster than GPT-NEO and\n",
        "GPT-NEO to be faster than GPT-2 124M. GPT-2 and GPT-NEO have similar quality where GPT-neo has better performance. \n",
        "Uncomment the model you want to use!\n",
        "'''\n",
        "# ai = aitextgen(tf_gpt2=\"124M\", to_gpu=True) #will download model into current directory.\n",
        "# ai_tuned = aitextgen(model=\"EleutherAI/gpt-neo-125M\", to_gpu=True) #will download model into current directory.\n",
        "ai_tuned = aitextgen(model=\"distilgpt2\", to_gpu=True) #will download model into current directory."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finetune GPT-2\n",
        "\n",
        "The next cell will start the actual finetuning of GPT-2 in aitextgen. It runs for `num_steps`, and a progress bar will appear to show training progress, current loss (the lower the better the model), and average loss (to give a sense on loss trajectory).\n",
        "\n",
        "The model will be saved every `save_every` steps in `trained_model` by default, and when training completes. If you mounted your Google Drive, the model will _also_ be saved there in a unique folder.\n",
        "\n",
        "The training might time out after 4ish hours; if you did not mount to Google Drive, make sure you end training and save the results so you don't lose them! (if this happens frequently, you may want to consider using [Colab Pro](https://colab.research.google.com/signup))\n",
        "\n",
        "Important parameters for `train()`:\n",
        "\n",
        "- **`line_by_line`**: Set this to `True` if the input text file is a single-column CSV, with one record per row. aitextgen will automatically process it optimally.\n",
        "- **`from_cache`**: If you compressed your dataset locally (as noted in the previous section) and are using that cache file, set this to `True`.\n",
        "- **`num_steps`**: Number of steps to train the model for.\n",
        "- **`generate_every`**: Interval of steps to generate example text from the model; good for qualitatively validating training.\n",
        "- **`save_every`**: Interval of steps to save the model: the model will be saved in the VM to `/trained_model`.\n",
        "- **`save_gdrive`**: Set this to `True` to copy the model to a unique folder in your Google Drive, if you have mounted it in the earlier cells\n",
        "- **`fp16`**: Enables half-precision training for faster/more memory-efficient training. Only works on a T4 or V100 GPU.\n",
        "\n",
        "Here are other important parameters for `train()` that are useful but you likely do not need to change.\n",
        "\n",
        "- **`learning_rate`**: Learning rate of the model training.\n",
        "- **`batch_size`**: Batch size of the model training; setting it too high will cause the GPU to go OOM. (if using `fp16`, you can increase the batch size more safely)"
      ],
      "metadata": {
        "id": "KG1sG_HFDMae"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPXzTwzrAMT3"
      },
      "outputs": [],
      "source": [
        "ai_tuned.train(training_file,\n",
        "         line_by_line=False,\n",
        "         from_cache=True,\n",
        "         num_steps=3000,\n",
        "         generate_every=500,\n",
        "         save_every=500,\n",
        "         save_gdrive=True,\n",
        "         learning_rate=1e-3,\n",
        "         batch_size=1,\n",
        "         output_dir = \"tuned_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "OTjQbDI1Jh-O"
      },
      "source": [
        "# Inference, Generation and Results\n",
        "Now that our models are trained, let's see what their output looks like in terms of lyric generation! The following code is very simple but will how to load in models and generate text in seconds!\n",
        "\n",
        "Your best resource for learning how to generate text can be found on [aitextgen's documentation](https://docs.aitextgen.io/generate/)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Loading in Models"
      ],
      "metadata": {
        "id": "rrx9UuXZWBhx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFaFLu-YJkDu"
      },
      "outputs": [],
      "source": [
        "#LOAD IN A MODEL, let's use the custom model\n",
        "ai = aitextgen(model_folder=\"custom_model\", tokenizer_file=\"/content/drive/MyDrive/LYRICBOT/aitextgen.tokenizer.json\", to_gpu=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating Lyrics"
      ],
      "metadata": {
        "id": "cu4sd8_lWD8c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thClP7zGJtmA"
      },
      "outputs": [],
      "source": [
        "ai.generate(n=1, batch_size=5, prompt=\"I wanna dance with somebody\", max_length=500, temperature=1, top_p=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n",
        "In classification, we use metrics like accuracy, precision, recall, F1-score, the list goes on...\n",
        "\n",
        "In text generation, it is very hard to quantify how well a model is generating text based on a prompt. For this reason, most use cases will require manual inspection of output (reading and judging the quality), and this evaluation method will be the norm for many teams at camp. \n",
        "\n",
        "With that said, there are metrics that do exist, and this example will walk you through how to use the BLEU metric to see how well your model generates text given a prompt. \n",
        "\n",
        "Note: This code was adapted from https://towardsdatascience.com/how-to-fine-tune-gpt-2-for-text-generation-ae2ea53bc272. You can read more about the logic on that page!"
      ],
      "metadata": {
        "id": "4fpXGbCfNRHj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***NOTE: BLEU (BiLingual Evaluation Understudy) is a metric for automatically evaluating machine-translated text. The BLEU score is a number between zero and one that measures the similarity of the machine-translated text to a set of high quality reference translations.***"
      ],
      "metadata": {
        "id": "Ooubrodvm6QD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_generation(ai, test_data):\n",
        "  generated_lyrics = []\n",
        "  for i in range(len(test_data)):\n",
        "    if(len(test_data[\"Lyric\"][i]) == 0):\n",
        "      continue\n",
        "    try:\n",
        "      x = ai.generate(n=1, prompt=test_data[\"Lyric\"][i], max_length=2000, temperature=0.7, return_as_list=True,  nonempty_output= False)\n",
        "      generated_lyrics.append(x[0])\n",
        "    except AssertionError as error:\n",
        "      # Output expected AssertionErrors.\n",
        "      generated_lyrics.append(np.NaN)\n",
        "      # print(\"Exception thrown: \", error)\n",
        "    except Exception as exception:\n",
        "      # Output unexpected Exceptions.\n",
        "      generated_lyrics.append(np.NaN)\n",
        "      # print(\"Exception thrown: \", exception)\n",
        "  return generated_lyrics"
      ],
      "metadata": {
        "id": "OYlRySJeE5eT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # this code was adapted from https://towardsdatascience.com/how-to-fine-tune-gpt-2-for-text-generation-ae2ea53bc272\n",
        "import statistics\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "test_set = pd.DataFrame(test[\"Lyric\"])\n",
        "test_set['True_end_lyrics'] = test_set['Lyric'].str.split().str[20:40].apply(' '.join)\n",
        "test_set['Lyric'] = test_set['Lyric'].str.split().str[0:10].apply(' '.join)\n",
        "\n",
        "generated_lyrics = text_generation(ai, test_set)\n"
      ],
      "metadata": {
        "id": "-62v5MUENXM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_set['Generated_lyrics'] = pd.Series(generated_lyrics)\n",
        "test_set.dropna(inplace=True)\n",
        "final_set = test_set.reset_index()\n",
        "final_set.head()\n",
        "\n",
        "scores = []\n",
        "samples_count = len(final_set)\n",
        "\n",
        "for i in range(len(final_set)):\n",
        "  reference = final_set['True_end_lyrics'][i]\n",
        "  candidate = final_set['Generated_lyrics'][i]\n",
        "  scores.append(sentence_bleu(reference, candidate))\n",
        "\n",
        "score = statistics.mean(scores)\n",
        "\n",
        "print(f'\\n\\nOverall BLEU score for this model, ran on a set of {samples_count} testing samples: {score}')"
      ],
      "metadata": {
        "id": "GIDpCb7aQ5M_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "cutts_aitextgen_tutorial.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}